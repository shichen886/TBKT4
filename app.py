import os
import sys

# åœ¨å¯¼å…¥ä»»ä½•åº“ä¹‹å‰ï¼Œå…ˆè®¾ç½®ç¯å¢ƒå˜é‡
app_dir = os.path.dirname(os.path.abspath(__file__))
paddlex_cache_dir = os.path.join(app_dir, 'paddlex_cache')
modelscope_cache_dir = os.path.join(app_dir, 'modelscope_cache')
os.makedirs(paddlex_cache_dir, exist_ok=True)
os.makedirs(modelscope_cache_dir, exist_ok=True)

# è®¾ç½®PaddleOCRç¼“å­˜ç›®å½•åˆ°åº”ç”¨ç›®å½•ï¼Œé¿å…æƒé™é—®é¢˜
os.environ['PADDLEX_HOME'] = paddlex_cache_dir
os.environ['PADDLE_HOME'] = paddlex_cache_dir
os.environ['MODELSCOPE_CACHE'] = modelscope_cache_dir
os.environ['PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK'] = 'True'
os.environ['PADDLEOCR_OFFLINE'] = 'True'

# è®¾ç½®PaddlePaddleä½¿ç”¨GPUï¼Œä½†ä¸ä¸PyTorchå†²çª
# å°è¯•è®¾ç½®PaddlePaddleç‰¹å®šçš„ç¯å¢ƒå˜é‡æ¥é¿å…å†²çª
os.environ['FLAGS_allocator_strategy'] = 'auto_growth'  # è‡ªåŠ¨å¢é•¿å†…å­˜åˆ†é…
os.environ['FLAGS_fraction_of_gpu_memory_to_use'] = '0.5'  # åªä½¿ç”¨50%çš„GPUå†…å­˜
os.environ['FLAGS_use_mkldnn'] = '0'  # ç¦ç”¨MKLDNNé¿å…å†²çª

import streamlit as st
import pandas as pd
import numpy as np
import torch
import shutil
import tempfile
from torch.nn.utils.rnn import pad_sequence
import plotly.graph_objects as go
import plotly.express as px

# å…¨å±€å˜é‡
PADDLEOCR_AVAILABLE = False
global_ocr_engine = None


from model_sakt import SAKT
from model_tsakt import TSAKT
from name_mappings import (
    load_mappings, save_mappings,
    get_user_name, get_item_name, get_skill_name,
    set_user_name, set_item_name, set_skill_name,
    auto_generate_skill_names
)
from recommendation import CollaborativeFiltering, ContentBasedRecommender, HybridRecommender
from learning_path import AdaptiveLearningPath, LearningPathOptimizer
from chart_config import ChartConfig

st.set_page_config(
    page_title="çŸ¥è¯†è¿½è¸ªç³»ç»Ÿ",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

with open('style.css', encoding='utf-8') as f:
    st.markdown(f'<style>{f.read()}</style>', unsafe_allow_html=True)

st.title("ğŸ“š æ™ºèƒ½çŸ¥è¯†è¿½è¸ªç³»ç»Ÿ")
st.markdown("---")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def get_data(df, max_length):
    item_ids = [torch.tensor(u_df["item_id"].values, dtype=torch.long)
                for _, u_df in df.groupby("user_id")]
    skill_ids = [torch.tensor(u_df["skill_id"].values, dtype=torch.long)
                 for _, u_df in df.groupby("user_id")]
    labels = [torch.tensor(u_df["correct"].values, dtype=torch.long)
              for _, u_df in df.groupby("user_id")]

    item_inputs = [torch.cat((torch.zeros(1, dtype=torch.long), i + 1))[:-1] for i in item_ids]
    skill_inputs = [torch.cat((torch.zeros(1, dtype=torch.long), s + 1))[:-1] for s in skill_ids]
    label_inputs = [torch.cat((torch.zeros(1, dtype=torch.long), l))[:-1] for l in labels]

    def chunk(list):
        if list[0] is None:
            return list
        list = [torch.split(elem, max_length) for elem in list]
        return [elem for sublist in list for elem in sublist]

    lists = (item_inputs, skill_inputs, label_inputs, item_ids, skill_ids, labels)
    chunked_lists = [chunk(l) for l in lists]

    data = list(zip(*chunked_lists))
    return data

def prepare_single_sequence(item_ids, skill_ids, labels):
    item_inputs = torch.cat((torch.zeros(1, dtype=torch.long), item_ids + 1))[:-1]
    skill_inputs = torch.cat((torch.zeros(1, dtype=torch.long), skill_ids + 1))[:-1]
    label_inputs = torch.cat((torch.zeros(1, dtype=torch.long), labels))[:-1]
    
    return item_inputs.unsqueeze(0), skill_inputs.unsqueeze(0), label_inputs.unsqueeze(0), \
           item_ids.unsqueeze(0), skill_ids.unsqueeze(0), labels.unsqueeze(0)

def load_models(dataset):
    df = pd.read_csv(os.path.join('data', dataset, 'preprocessed_data.csv'), sep="\t")
    num_items = int(df["item_id"].max() + 1)
    num_skills = int(df["skill_id"].max() + 1)
    
    sakt_path = os.path.join('save/sakt', f'{dataset},batch_size=128,max_length=200,encode_pos=False,max_pos=10')
    if not os.path.exists(sakt_path):
        sakt_path = os.path.join('save/sakt', f'{dataset},batch_size=128,max_length=200,encode_pos=False,max_pos=5')
    
    if os.path.exists(sakt_path):
        loaded_model = torch.load(sakt_path, map_location=device, weights_only=False)
        sakt_model = loaded_model.to(device)
        sakt_model.eval()
    else:
        sakt_model = None
    
    tsakt_path = os.path.join('save/tsakt', f'{dataset},batch_size=128,max_length=200,encode_pos=False,max_pos=5,tensor_rank=3')
    
    if os.path.exists(tsakt_path):
        loaded_model = torch.load(tsakt_path, map_location=device, weights_only=False)
        tsakt_model = loaded_model.to(device)
        tsakt_model.eval()
        tsakt_available = True
    else:
        tsakt_model = None
        tsakt_available = False
    
    return sakt_model, tsakt_model, df, num_items, num_skills, tsakt_available

def predict_next_question(model, item_ids, skill_ids, labels):
    item_inputs, skill_inputs, label_inputs, item_ids_batch, skill_ids_batch, labels_batch = \
        prepare_single_sequence(item_ids, skill_ids, labels)
    
    item_inputs = item_inputs.to(device)
    skill_inputs = skill_inputs.to(device)
    label_inputs = label_inputs.to(device)
    item_ids_batch = item_ids_batch.to(device)
    skill_ids_batch = skill_ids_batch.to(device)
    
    with torch.no_grad():
        preds = model(item_inputs, skill_inputs, label_inputs, item_ids_batch, skill_ids_batch)
        preds = torch.sigmoid(preds).cpu().numpy()
    
    return preds[0, -1].item()

def analyze_student_performance(df, user_id):
    user_data = df[df['user_id'] == user_id].sort_values('item_id')
    
    if len(user_data) == 0:
        return None
    
    skill_stats = user_data.groupby('skill_id').agg({
        'correct': ['mean', 'count']
    }).reset_index()
    skill_stats.columns = ['skill_id', 'accuracy', 'count']
    
    # ä¿®æ”¹æŒæ¡åº¦è®¡ç®—æ–¹å¼ï¼šç»¼åˆè€ƒè™‘æ­£ç¡®ç‡å’Œç­”é¢˜æ•°é‡
    # æŒæ¡åº¦ = æ­£ç¡®ç‡ * (1 - 1/(1 + count/10)) 
    # è¿™æ ·ç­”é¢˜æ•°é‡è¶Šå¤šï¼Œæƒé‡è¶Šé«˜ï¼Œä½†ä¸ä¼šæ— é™å¢é•¿
    skill_stats['mastery'] = skill_stats['accuracy'] * (1 - 1/(1 + skill_stats['count']/10))
    
    return skill_stats

def recommend_questions(skill_stats, num_questions=5, difficulty='balanced'):
    if skill_stats is None or len(skill_stats) == 0:
        return []
    
    if difficulty == 'easy':
        recommended = skill_stats.nlargest(num_questions, 'accuracy')
    elif difficulty == 'hard':
        recommended = skill_stats.nsmallest(num_questions, 'accuracy')
    else:
        recommended = skill_stats.nsmallest(num_questions, 'mastery')
    
    return recommended['skill_id'].tolist()

st.sidebar.title("âš™ï¸ ç³»ç»Ÿè®¾ç½®")

available_datasets = []
for folder in os.listdir('data'):
    if os.path.isdir(os.path.join('data', folder)):
        if os.path.exists(os.path.join('data', folder, 'preprocessed_data.csv')):
            available_datasets.append(folder)

dataset = st.sidebar.selectbox(
    "é€‰æ‹©æ•°æ®é›†",
    sorted(available_datasets)
)

st.sidebar.markdown("---")

st.sidebar.header("ğŸ“Š æ¨¡å‹æ€§èƒ½")

sakt_model, tsakt_model, df, num_items, num_skills, tsakt_available = load_models(dataset)

mappings = load_mappings()

st.sidebar.metric("SAKT AUC", "0.7769")
st.sidebar.metric("TSAKT AUC", "0.7843", delta="+0.0074")

st.sidebar.markdown("---")

st.sidebar.header("ğŸ“ˆ ç³»ç»ŸåŠŸèƒ½")

model_choice = st.sidebar.selectbox(
    "é€‰æ‹©æ¨¡å‹",
    ["SAKT"] + (["TSAKT"] if tsakt_available else [])
)

st.sidebar.markdown("---")

st.sidebar.header("ğŸ¤– æ¨èç®—æ³•")

recommendation_method = st.sidebar.selectbox(
    "æ¨èæ–¹æ³•",
    ["æ··åˆæ¨è", "ååŒè¿‡æ»¤", "åŸºäºå†…å®¹", "ä¼ ç»Ÿæ–¹æ³•"]
)

if recommendation_method != "ä¼ ç»Ÿæ–¹æ³•":
    try:
        if recommendation_method == "æ··åˆæ¨è":
            recommender = HybridRecommender()
        elif recommendation_method == "ååŒè¿‡æ»¤":
            recommender = CollaborativeFiltering()
        else:
            recommender = ContentBasedRecommender()
        
        recommender.fit(df)
        st.sidebar.success("âœ… æ¨èæ¨¡å‹å·²åŠ è½½")
    except Exception as e:
        st.sidebar.warning(f"âš ï¸ æ¨èæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        recommender = None
else:
    recommender = None

st.sidebar.markdown("---")

st.sidebar.header("ğŸ—ºï¸ å­¦ä¹ è·¯å¾„ä¼˜åŒ–")

enable_adaptive_path = st.sidebar.checkbox("å¯ç”¨è‡ªé€‚åº”å­¦ä¹ è·¯å¾„", value=True)

if enable_adaptive_path:
    try:
        learning_path_optimizer = LearningPathOptimizer(df)
        st.sidebar.success("âœ… å­¦ä¹ è·¯å¾„ä¼˜åŒ–å™¨å·²åŠ è½½")
    except Exception as e:
        st.sidebar.warning(f"âš ï¸ å­¦ä¹ è·¯å¾„ä¼˜åŒ–å™¨åŠ è½½å¤±è´¥: {str(e)}")
        learning_path_optimizer = None
else:
    learning_path_optimizer = None

tabs = st.tabs([
    "ğŸ¯ ä¸ªæ€§åŒ–å­¦ä¹ æ¨è",
    "ğŸ“Š å­¦ç”Ÿå­¦ä¹ åˆ†æ",
    "ğŸ—ºï¸ å­¦ä¹ è·¯å¾„ä¼˜åŒ–",
    "ğŸ“ æ•™è‚²è¯„ä¼°",
    "ğŸ“¤ ä¸Šä¼ æ•°æ®"
])

with tabs[0]:
    st.header("ğŸ¯ ä¸ªæ€§åŒ–å­¦ä¹ æ¨è")
    st.markdown("æ ¹æ®å­¦ç”Ÿå†å²ç­”é¢˜è®°å½•ï¼Œæ™ºèƒ½æ¨èé€‚åˆçš„é¢˜ç›®")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("å­¦ç”Ÿä¿¡æ¯")
        
        unique_users = df['user_id'].unique()
        user_options = {get_user_name(mappings, uid): uid for uid in unique_users}
        selected_user_name = st.selectbox("é€‰æ‹©å­¦ç”Ÿ", sorted(user_options.keys()))
        user_id = user_options[selected_user_name]
        
        user_data = df[df['user_id'] == user_id]
        
        if len(user_data) > 0:
            st.write(f"æ€»ç­”é¢˜æ•°: {len(user_data)}")
            st.write(f"æ­£ç¡®ç‡: {user_data['correct'].mean():.2%}")
            st.write(f"æ¶‰åŠçŸ¥è¯†ç‚¹: {user_data['skill_id'].nunique()}")
        else:
            st.warning("æœªæ‰¾åˆ°è¯¥å­¦ç”Ÿçš„æ•°æ®")
    
    with col2:
        st.subheader("æ¨èè®¾ç½®")
        difficulty = st.selectbox(
            "æ¨èéš¾åº¦",
            ["balanced", "easy", "hard"],
            index=0,
            format_func=lambda x: {"balanced": "å‡è¡¡ï¼ˆæ¨èè–„å¼±çŸ¥è¯†ç‚¹ï¼‰", "easy": "ç®€å•ï¼ˆå·©å›ºå·²æŒæ¡ï¼‰", "hard": "å›°éš¾ï¼ˆæŒ‘æˆ˜é«˜éš¾åº¦ï¼‰"}[x]
        )
        num_questions = st.slider("æ¨èé¢˜ç›®æ•°é‡", 1, 10, 5)
    
    if len(user_data) > 0:
        st.markdown("---")
        st.subheader("ğŸ“‹ æ¨èé¢˜ç›®")
        
        if recommender is not None:
            recommended_items = recommender.recommend_for_user(user_id, num_questions)
            
            if recommended_items:
                for i, item_id in enumerate(recommended_items, 1):
                    item_data = df[df['item_id'] == item_id].iloc[0]
                    skill_id = item_data['skill_id']
                    skill_name = get_skill_name(mappings, skill_id)
                    
                    skill_stats = analyze_student_performance(df, user_id)
                    if skill_stats is not None and skill_id in skill_stats['skill_id'].values:
                        skill_data = skill_stats[skill_stats['skill_id'] == skill_id].iloc[0]
                        mastery = skill_data['mastery']
                        accuracy = skill_data['accuracy']
                    else:
                        mastery = 0
                        accuracy = 0
                    
                    col_a, col_b, col_c = st.columns([1, 2, 1])
                    with col_a:
                        st.metric(f"é¢˜ç›® {i}", skill_name)
                    with col_b:
                        progress = mastery * 100
                        st.progress(progress / 100)
                        st.caption(f"æŒæ¡åº¦: {mastery:.2%} | æ­£ç¡®ç‡: {accuracy:.2%}")
                    with col_c:
                        if accuracy < 0.5:
                            st.error("éœ€åŠ å¼º")
                        elif accuracy < 0.7:
                            st.warning("ä¸€èˆ¬")
                        else:
                            st.success("è‰¯å¥½")
            else:
                st.info("æš‚æ— æ¨èé¢˜ç›®")
                st.caption("ğŸ’¡ æç¤ºï¼šæ¨èç®—æ³•å¯èƒ½å› ä¸ºä»¥ä¸‹åŸå› æ— æ³•ç”Ÿæˆæ¨èï¼š")
                st.caption("1. å½“å‰å­¦ç”Ÿä¸åœ¨æ´»è·ƒç”¨æˆ·åˆ—è¡¨ä¸­ï¼ˆæ¨èç®—æ³•åªå¤„ç†æœ€æ´»è·ƒçš„500ä¸ªç”¨æˆ·ï¼‰")
                st.caption("2. å­¦ç”Ÿç­”é¢˜çš„é¢˜ç›®ä¸åœ¨æ¨èç®—æ³•å¤„ç†çš„é¢˜ç›®èŒƒå›´å†…ï¼ˆåªå¤„ç†æœ€æ´»è·ƒçš„500ä¸ªé¢˜ç›®ï¼‰")
                st.caption("3. å»ºè®®åˆ‡æ¢åˆ°'ä¼ ç»Ÿæ–¹æ³•'æ¨èï¼Œæˆ–é€‰æ‹©ç­”é¢˜æ•°è¾ƒå¤šçš„å­¦ç”Ÿ")
        else:
            skill_stats = analyze_student_performance(df, user_id)
            recommended_skills = recommend_questions(skill_stats, num_questions, difficulty)
            
            if recommended_skills:
                for i, skill_id in enumerate(recommended_skills, 1):
                    skill_data = skill_stats[skill_stats['skill_id'] == skill_id].iloc[0]
                    mastery = skill_data['mastery']
                    accuracy = skill_data['accuracy']
                    skill_name = get_skill_name(mappings, skill_id)
                    
                    col_a, col_b, col_c = st.columns([1, 2, 1])
                    with col_a:
                        st.metric(f"é¢˜ç›® {i}", skill_name)
                    with col_b:
                        progress = mastery * 100
                        st.progress(progress / 100)
                        st.caption(f"æŒæ¡åº¦: {mastery:.2%} | æ­£ç¡®ç‡: {accuracy:.2%}")
                    with col_c:
                        if accuracy < 0.5:
                            st.error("éœ€åŠ å¼º")
                        elif accuracy < 0.7:
                            st.warning("ä¸€èˆ¬")
                        else:
                            st.success("è‰¯å¥½")
            else:
                st.info("æš‚æ— æ¨èé¢˜ç›®")
        
        st.markdown("---")
        st.subheader("ğŸ”® é¢„æµ‹ä¸‹ä¸€é¢˜")
        
        model = sakt_model if model_choice == "SAKT" else tsakt_model
        
        if model is None:
            st.warning(f"âš ï¸ {model_choice} æ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·é€‰æ‹©å…¶ä»–æ¨¡å‹")
        else:
            recent_answers = user_data.tail(10)
            item_ids = torch.tensor(recent_answers['item_id'].values, dtype=torch.long)
            skill_ids = torch.tensor(recent_answers['skill_id'].values, dtype=torch.long)
            labels = torch.tensor(recent_answers['correct'].values, dtype=torch.long)
            
            prediction = predict_next_question(model, item_ids, skill_ids, labels)
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("é¢„æµ‹æ­£ç¡®ç‡", f"{prediction:.2%}")
            with col_b:
                if prediction > 0.7:
                    st.success("å‡†å¤‡å……åˆ†")
                elif prediction > 0.5:
                    st.warning("éœ€è¦å¤ä¹ ")
                else:
                    st.error("å»ºè®®å…ˆå­¦ä¹ ")

with tabs[1]:
    st.header("ğŸ“Š å­¦ç”Ÿå­¦ä¹ åˆ†æ")
    st.markdown("æ·±å…¥åˆ†æå­¦ç”Ÿçš„å­¦ä¹ æƒ…å†µå’Œè–„å¼±ç¯èŠ‚")
    
    col1, col2 = st.columns(2)
    
    with col1:
        unique_users = df['user_id'].unique()
        user_options = {get_user_name(mappings, uid): uid for uid in unique_users}
        selected_user_name = st.selectbox("é€‰æ‹©å­¦ç”Ÿ", sorted(user_options.keys()), key="analysis_user")
        user_id = user_options[selected_user_name]
    
    with col2:
        analysis_type = st.selectbox(
            "åˆ†æç±»å‹",
            ["çŸ¥è¯†ç‚¹æŒæ¡æƒ…å†µ", "ç­”é¢˜è¶‹åŠ¿", "é”™è¯¯åˆ†æ"]
        )
    
    user_data = df[df['user_id'] == user_id]
    
    if len(user_data) > 0:
        if analysis_type == "çŸ¥è¯†ç‚¹æŒæ¡æƒ…å†µ":
            st.subheader("çŸ¥è¯†ç‚¹æŒæ¡æƒ…å†µ")
            
            skill_stats = analyze_student_performance(df, user_id)
            
            if skill_stats is not None and len(skill_stats) > 0:
                skill_stats['skill_name'] = skill_stats['skill_id'].apply(lambda x: get_skill_name(mappings, x))
                fig = ChartConfig.create_bar_chart(
                    skill_stats,
                    x_col='skill_name',
                    y_col='accuracy',
                    title='å„çŸ¥è¯†ç‚¹æ­£ç¡®ç‡',
                    color_col='accuracy',
                    labels={'skill_name': 'çŸ¥è¯†ç‚¹', 'accuracy': 'æ­£ç¡®ç‡'}
                )
                st.plotly_chart(fig, use_container_width=True)
                
                st.subheader("è–„å¼±çŸ¥è¯†ç‚¹è¯†åˆ«")
                weak_skills = skill_stats.nsmallest(5, 'accuracy')
                
                for _, row in weak_skills.iterrows():
                    col_a, col_b, col_c = st.columns([1, 2, 1])
                    with col_a:
                        st.write(get_skill_name(mappings, int(row['skill_id'])))
                    with col_b:
                        st.progress(row['accuracy'])
                    with col_c:
                        st.error(f"{row['accuracy']:.2%}")
        
        elif analysis_type == "ç­”é¢˜è¶‹åŠ¿":
            st.subheader("ç­”é¢˜è¶‹åŠ¿åˆ†æ")
            
            user_data_sorted = user_data.sort_values('item_id')
            user_data_sorted['cumulative_accuracy'] = user_data_sorted['correct'].expanding().mean()
            
            fig = ChartConfig.create_line_chart(
                user_data_sorted,
                y_col='cumulative_accuracy',
                title='ç´¯è®¡æ­£ç¡®ç‡è¶‹åŠ¿',
                x_col='index',
                labels={'index': 'ç­”é¢˜åºå·', 'cumulative_accuracy': 'ç´¯è®¡æ­£ç¡®ç‡'}
            )
            fig.add_hline(y=0.7, line_dash="dash", line_color="#EF4444", annotation_text="ç›®æ ‡çº¿ 70%")
            st.plotly_chart(fig, use_container_width=True)
            
            st.subheader("å­¦ä¹ è¿›åº¦")
            total_questions = len(user_data)
            correct_questions = user_data['correct'].sum()
            
            col_a, col_b = st.columns(2)
            with col_a:
                st.metric("æ€»ç­”é¢˜æ•°", total_questions)
            with col_b:
                st.metric("æ­£ç¡®ç­”é¢˜æ•°", correct_questions)
            
            st.progress(correct_questions / total_questions)
        
        elif analysis_type == "é”™è¯¯åˆ†æ":
            st.subheader("é”™è¯¯åˆ†æ")
            
            wrong_answers = user_data[user_data['correct'] == 0]
            
            if len(wrong_answers) > 0:
                skill_error_counts = wrong_answers['skill_id'].value_counts().head(10)
                
                fig = ChartConfig.create_pie_chart(
                    values=skill_error_counts.values,
                    names=[get_skill_name(mappings, int(k)) for k in skill_error_counts.index],
                    title='é”™è¯¯åˆ†å¸ƒ'
                )
                st.plotly_chart(fig, use_container_width=True)
                
                st.subheader("éœ€è¦é‡ç‚¹å¤ä¹ çš„çŸ¥è¯†ç‚¹")
                for skill_id, count in skill_error_counts.items():
                    st.write(f"{get_skill_name(mappings, int(skill_id))}: {count} æ¬¡é”™è¯¯")
            else:
                st.success("ğŸ‰ æ­å–œï¼è¯¥å­¦ç”Ÿæ²¡æœ‰é”™è¯¯è®°å½•")
    else:
        st.warning("æœªæ‰¾åˆ°è¯¥å­¦ç”Ÿçš„æ•°æ®")

with tabs[2]:
    st.header("ğŸ—ºï¸ å­¦ä¹ è·¯å¾„ä¼˜åŒ–")
    st.markdown("åˆ¶å®šä¸ªæ€§åŒ–çš„å­¦ä¹ è®¡åˆ’ï¼Œä¼˜åŒ–å­¦ä¹ è·¯å¾„")
    
    col1, col2 = st.columns(2)
    
    with col1:
        unique_users = df['user_id'].unique()
        user_options = {get_user_name(mappings, uid): uid for uid in unique_users}
        selected_user_name = st.selectbox("é€‰æ‹©å­¦ç”Ÿ", sorted(user_options.keys()), key="path_user")
        user_id = user_options[selected_user_name]
    
    with col2:
        learning_goal = st.selectbox(
            "å­¦ä¹ ç›®æ ‡",
            ["å…¨é¢æŒæ¡", "é‡ç‚¹çªç ´", "æŸ¥æ¼è¡¥ç¼º"]
        )
    
    user_data = df[df['user_id'] == user_id]
    
    if len(user_data) > 0:
        st.subheader("ğŸ“… ä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’")
        
        if learning_path_optimizer is not None and learning_path_optimizer.adaptive_path is not None:
            # è‡ªé€‚åº”å­¦ä¹ è·¯å¾„
            learning_path = learning_path_optimizer.adaptive_path.recommend_learning_path(user_id, max_length=10)
            
            if learning_path:
                for i, skill_id in enumerate(learning_path, 1):
                    skill_name = get_skill_name(mappings, skill_id)
                    skill_data = df[df['skill_id'] == skill_id]
                    
                    with st.expander(f"ç¬¬ {i} é˜¶æ®µ: {skill_name}"):
                        # è·å–å­¦ç”Ÿåœ¨è¯¥çŸ¥è¯†ç‚¹ä¸Šçš„è¡¨ç°
                        user_skill_data = user_data[user_data['skill_id'] == skill_id]
                        if len(user_skill_data) > 0:
                            accuracy = user_skill_data['correct'].mean()
                            count = len(user_skill_data)
                        else:
                            accuracy = 0
                            count = 0
                        
                        col_a, col_b = st.columns(2)
                        with col_a:
                            st.metric("å½“å‰æ­£ç¡®ç‡", f"{accuracy:.2%}")
                        with col_b:
                            st.metric("ç»ƒä¹ æ¬¡æ•°", count)
                        
                        st.progress(accuracy)
                        
                        # é¢„æµ‹å­¦ä¹ æ—¶é—´
                        predicted_time = learning_path_optimizer.predict_learning_time(user_id, skill_id)
                        st.info(f"â±ï¸ é¢„è®¡å­¦ä¹ æ—¶é—´: {predicted_time:.1f} å°æ—¶")
                        
                        # è·å–å‰ç½®çŸ¥è¯†ç‚¹
                        prerequisites = learning_path_optimizer.adaptive_path.get_skill_prerequisites(skill_id)
                        if prerequisites:
                            st.write(f"ğŸ“š å‰ç½®çŸ¥è¯†ç‚¹: {', '.join([get_skill_name(mappings, p) for p in prerequisites])}")
                        else:
                            st.caption("ğŸ“š å‰ç½®çŸ¥è¯†ç‚¹: æ— ï¼ˆè¯¥çŸ¥è¯†ç‚¹å¯ç‹¬ç«‹å­¦ä¹ ï¼‰")
                        
                        # ç”Ÿæˆå­¦ä¹ ä»»åŠ¡
                        tasks = learning_path_optimizer._generate_tasks(skill_id)
                        st.write("ğŸ“‹ å­¦ä¹ ä»»åŠ¡:")
                        for task in tasks:
                            st.write(f"  â€¢ {task}")
                
                st.markdown("---")
                st.subheader("ğŸ“Š å­¦ä¹ è·¯å¾„å¯è§†åŒ–")
                
                fig = go.Figure()
                
                for i, skill_id in enumerate(learning_path, 1):
                    skill_name = get_skill_name(mappings, skill_id)
                    user_skill_data = user_data[user_data['skill_id'] == skill_id]
                    accuracy = user_skill_data['correct'].mean() if len(user_skill_data) > 0 else 0
                    
                    fig.add_trace(go.Bar(
                        x=[skill_name],
                        y=[accuracy],
                        name=f"é˜¶æ®µ {i}",
                        marker_color=['red' if accuracy < 0.5 else 'orange' if accuracy < 0.7 else 'green'][0]
                    ))
                
                fig.update_layout(
                    title="è‡ªé€‚åº”å­¦ä¹ è·¯å¾„è§„åˆ’",
                    xaxis_title="çŸ¥è¯†ç‚¹",
                    yaxis_title="æ­£ç¡®ç‡",
                    yaxis_range=[0, 1],
                    showlegend=False
                )
                
                st.plotly_chart(fig, width='stretch')
            else:
                st.info("ğŸ’¡ å½“å‰å­¦ç”Ÿä¸åœ¨è‡ªé€‚åº”å­¦ä¹ è·¯å¾„çš„æ´»è·ƒç”¨æˆ·åˆ—è¡¨ä¸­ï¼ˆåªå¤„ç†æœ€æ´»è·ƒçš„500ä¸ªç”¨æˆ·ï¼‰")
                st.caption("å»ºè®®ï¼šé€‰æ‹©ç­”é¢˜æ•°è¾ƒå¤šçš„å­¦ç”Ÿï¼Œæˆ–å…³é—­è‡ªé€‚åº”å­¦ä¹ è·¯å¾„åŠŸèƒ½")
        elif learning_path_optimizer is not None and learning_path_optimizer.adaptive_path is None:
            # è‡ªé€‚åº”å­¦ä¹ è·¯å¾„åˆå§‹åŒ–å¤±è´¥ï¼Œæ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
            st.error("âŒ è‡ªé€‚åº”å­¦ä¹ è·¯å¾„æœªåˆå§‹åŒ–")
            if learning_path_optimizer.error_message:
                st.error(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {learning_path_optimizer.error_message}")
            st.info("ğŸ’¡ å»ºè®®å…³é—­è‡ªé€‚åº”å­¦ä¹ è·¯å¾„åŠŸèƒ½ï¼Œä½¿ç”¨æ™®é€šå­¦ä¹ è·¯å¾„è§„åˆ’")
        
        # æ™®é€šå­¦ä¹ è·¯å¾„ï¼ˆæ— è®ºæ˜¯å¦å¯ç”¨è‡ªé€‚åº”å­¦ä¹ è·¯å¾„ï¼Œéƒ½ä¼šæ˜¾ç¤ºï¼‰
        st.markdown("---")
        st.subheader("ğŸ“– æ™®é€šå­¦ä¹ è·¯å¾„è§„åˆ’")
        st.caption("åŸºäºæ­£ç¡®ç‡æ’åºçš„å­¦ä¹ è·¯å¾„ï¼Œä¸è€ƒè™‘çŸ¥è¯†ç‚¹ä¾èµ–å…³ç³»")
        skill_stats = analyze_student_performance(df, user_id)
        
        if skill_stats is not None and len(skill_stats) > 0:
            skill_stats['skill_name'] = skill_stats['skill_id'].apply(lambda x: get_skill_name(mappings, x))
            
            if learning_goal == "å…¨é¢æŒæ¡":
                sorted_skills = skill_stats.sort_values('accuracy')
            elif learning_goal == "é‡ç‚¹çªç ´":
                sorted_skills = skill_stats.nsmallest(5, 'accuracy')
            else:
                sorted_skills = skill_stats[skill_stats['accuracy'] < 0.7].sort_values('accuracy')
                
                if len(sorted_skills) == 0:
                    st.success("ğŸ‰ æ­å–œï¼ä½ å·²ç»æŒæ¡äº†æ‰€æœ‰çŸ¥è¯†ç‚¹ï¼ˆæ­£ç¡®ç‡å‡â‰¥70%ï¼‰")
                    sorted_skills = skill_stats.sort_values('accuracy')
            
            if len(sorted_skills) > 0:
                for i, (_, row) in enumerate(sorted_skills.iterrows(), 1):
                    with st.expander(f"ç¬¬ {i} é˜¶æ®µ: {row['skill_name']}"):
                        col_a, col_b = st.columns(2)
                        with col_a:
                            st.metric("å½“å‰æ­£ç¡®ç‡", f"{row['accuracy']:.2%}")
                        with col_b:
                            st.metric("ç»ƒä¹ æ¬¡æ•°", int(row['count']))
                        
                        st.progress(row['accuracy'])
                        
                        if row['accuracy'] < 0.5:
                            st.warning("âš ï¸ è¯¥çŸ¥è¯†ç‚¹æŒæ¡è¾ƒå·®ï¼Œå»ºè®®ä¼˜å…ˆå­¦ä¹ ")
                        elif row['accuracy'] < 0.7:
                            st.info("ğŸ“š è¯¥çŸ¥è¯†ç‚¹éœ€è¦åŠ å¼ºç»ƒä¹ ")
                        else:
                            st.success("âœ… è¯¥çŸ¥è¯†ç‚¹æŒæ¡è‰¯å¥½")
                
                st.markdown("---")
                st.subheader("ğŸ“Š å­¦ä¹ è·¯å¾„å¯è§†åŒ–")
                
                fig = go.Figure()
                
                for i, (_, row) in enumerate(sorted_skills.head(10).iterrows()):
                    fig.add_trace(go.Bar(
                        x=[row['skill_name']],
                        y=[row['accuracy']],
                        name=f"é˜¶æ®µ {i+1}",
                        marker_color=['red' if row['accuracy'] < 0.5 else 'orange' if row['accuracy'] < 0.7 else 'green'][0]
                    ))
                
                fig.update_layout(
                    title="å­¦ä¹ è·¯å¾„è§„åˆ’",
                    xaxis_title="çŸ¥è¯†ç‚¹",
                    yaxis_title="æ­£ç¡®ç‡",
                    yaxis_range=[0, 1],
                    showlegend=False
                )
                
                st.plotly_chart(fig, width='stretch')
    else:
        st.warning("æœªæ‰¾åˆ°è¯¥å­¦ç”Ÿçš„æ•°æ®")

with tabs[3]:
    st.header("ğŸ“ æ•™è‚²è¯„ä¼°")
    st.markdown("è¯„ä¼°æ•™å­¦æ•ˆæœï¼Œåˆ†æä¸åŒå­¦ç”Ÿçš„å­¦ä¹ æ¨¡å¼")
    
    col1, col2 = st.columns(2)
    
    with col1:
        evaluation_type = st.selectbox(
            "è¯„ä¼°ç±»å‹",
            ["æ•´ä½“æ•™å­¦æ•ˆæœ", "å­¦ç”Ÿç¾¤ä½“åˆ†æ", "çŸ¥è¯†ç‚¹éš¾åº¦åˆ†æ"]
        )
    
    with col2:
        num_students = st.slider("åˆ†æå­¦ç”Ÿæ•°é‡", 10, 100, 50)
    
    if evaluation_type == "æ•´ä½“æ•™å­¦æ•ˆæœ":
        st.subheader("æ•´ä½“æ•™å­¦æ•ˆæœè¯„ä¼°")
        
        sample_users = df['user_id'].unique()[:num_students]
        user_stats = []
        
        for user_id in sample_users:
            user_data = df[df['user_id'] == user_id]
            if len(user_data) > 0:
                user_stats.append({
                    'user_id': user_id,
                    'accuracy': user_data['correct'].mean(),
                    'total_questions': len(user_data)
                })
        
        user_stats_df = pd.DataFrame(user_stats)
        
        col_a, col_b, col_c = st.columns(3)
        with col_a:
            st.metric("å¹³å‡æ­£ç¡®ç‡", f"{user_stats_df['accuracy'].mean():.2%}")
        with col_b:
            st.metric("æœ€é«˜æ­£ç¡®ç‡", f"{user_stats_df['accuracy'].max():.2%}")
        with col_c:
            st.metric("æœ€ä½æ­£ç¡®ç‡", f"{user_stats_df['accuracy'].min():.2%}")
        
        fig = px.histogram(
            user_stats_df,
            x='accuracy',
            title='å­¦ç”Ÿæ­£ç¡®ç‡åˆ†å¸ƒ',
            labels={'accuracy': 'æ­£ç¡®ç‡', 'count': 'å­¦ç”Ÿæ•°é‡'},
            nbins=20
        )
        fig.add_vline(x=user_stats_df['accuracy'].mean(), line_dash="dash", line_color="red", 
                     annotation_text=f"å¹³å‡: {user_stats_df['accuracy'].mean():.2%}")
        st.plotly_chart(fig, width='stretch')
    
    elif evaluation_type == "å­¦ç”Ÿç¾¤ä½“åˆ†æ":
        st.subheader("å­¦ç”Ÿç¾¤ä½“åˆ†æ")
        
        sample_users = df['user_id'].unique()[:num_students]
        
        user_categories = {'ä¼˜ç§€': 0, 'è‰¯å¥½': 0, 'ä¸€èˆ¬': 0, 'éœ€åŠ å¼º': 0}
        
        for user_id in sample_users:
            user_data = df[df['user_id'] == user_id]
            if len(user_data) > 0:
                accuracy = user_data['correct'].mean()
                if accuracy >= 0.9:
                    user_categories['ä¼˜ç§€'] += 1
                elif accuracy >= 0.7:
                    user_categories['è‰¯å¥½'] += 1
                elif accuracy >= 0.5:
                    user_categories['ä¸€èˆ¬'] += 1
                else:
                    user_categories['éœ€åŠ å¼º'] += 1
        
        fig = go.Figure(data=[go.Pie(
            labels=list(user_categories.keys()),
            values=list(user_categories.values()),
            hole=.3
        )])
        
        fig.update_layout(
            title='å­¦ç”Ÿç¾¤ä½“åˆ†å¸ƒ',
            annotations=[dict(text='å­¦ç”Ÿåˆ†å¸ƒ', x=0.5, y=0.5, font_size=20, showarrow=False)]
        )
        
        st.plotly_chart(fig, width='stretch')
        
        for category, count in user_categories.items():
            st.write(f"{category}: {count} äºº ({count/num_students:.1%})")
    
    elif evaluation_type == "çŸ¥è¯†ç‚¹éš¾åº¦åˆ†æ":
        st.subheader("çŸ¥è¯†ç‚¹éš¾åº¦åˆ†æ")
        
        skill_stats = df.groupby('skill_id').agg({
            'correct': ['mean', 'count']
        }).reset_index()
        skill_stats.columns = ['skill_id', 'accuracy', 'count']
        
        skill_stats['skill_name'] = skill_stats['skill_id'].apply(lambda x: get_skill_name(mappings, x))
        
        skill_stats['difficulty'] = pd.cut(
            skill_stats['accuracy'],
            bins=[0, 0.5, 0.7, 0.9, 1],
            labels=['å›°éš¾', 'ä¸­ç­‰', 'ç®€å•', 'éå¸¸ç®€å•']
        )
        
        difficulty_counts = skill_stats['difficulty'].value_counts()
        
        fig = ChartConfig.create_bar_chart(
            pd.DataFrame({'difficulty': difficulty_counts.index, 'count': difficulty_counts.values}),
            x_col='difficulty',
            y_col='count',
            title='çŸ¥è¯†ç‚¹éš¾åº¦åˆ†å¸ƒ',
            labels={'difficulty': 'éš¾åº¦ç­‰çº§', 'count': 'çŸ¥è¯†ç‚¹æ•°é‡'}
        )
        st.plotly_chart(fig, use_container_width=True)
        
        st.subheader("å›°éš¾çŸ¥è¯†ç‚¹åˆ—è¡¨")
        difficult_skills = skill_stats[skill_stats['accuracy'] < 0.5].sort_values('accuracy')
        
        for _, row in difficult_skills.head(10).iterrows():
            col_a, col_b, col_c = st.columns([1, 2, 1])
            with col_a:
                st.write(row['skill_name'])
            with col_b:
                st.progress(row['accuracy'])
            with col_c:
                st.error(f"{row['accuracy']:.2%}")

with tabs[4]:
    st.header("ğŸ“¤ ä¸Šä¼ æ•°æ®")
    st.markdown("ä¸Šä¼ è‡ªå®šä¹‰çš„ä¹ é¢˜æ•°æ®æˆ–é€šè¿‡æ‹ç…§æ·»åŠ æ–°é¢˜ç›®")
    
    upload_type = st.radio(
        "é€‰æ‹©ä¸Šä¼ æ–¹å¼",
        ["CSV æ–‡ä»¶ä¸Šä¼ ", "æ‹ç…§/å›¾ç‰‡ä¸Šä¼ ", "è‡ªå®šä¹‰åç§°"],
        horizontal=True
    )
    
    if upload_type == "CSV æ–‡ä»¶ä¸Šä¼ ":
        st.subheader("ğŸ“‹ æ•°æ®æ ¼å¼è¦æ±‚")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **å¿…éœ€åˆ—ï¼š**
            | åˆ—å | ç±»å‹ | è¯´æ˜ |
            |--------|------|------|
            | `user_id` | æ•´æ•° | å­¦ç”ŸIDï¼ˆä»0å¼€å§‹ï¼‰ |
            | `item_id` | æ•´æ•° | é¢˜ç›®IDï¼ˆä»0å¼€å§‹ï¼‰ |
            | `correct` | 0æˆ–1 | ç­”é¢˜ç»“æœï¼ˆ0=é”™è¯¯ï¼Œ1=æ­£ç¡®ï¼‰ |
            | `skill_id` | æ•´æ•° | çŸ¥è¯†ç‚¹IDï¼ˆä»0å¼€å§‹ï¼‰ |
            """)
        
        with col2:
            st.markdown("""
            **å¯é€‰åˆ—ï¼š**
            | åˆ—å | ç±»å‹ | è¯´æ˜ |
            |--------|------|------|
            | `timestamp` | æ•´æ•° | ç­”é¢˜æ—¶é—´æˆ³ |
            """)
        
        st.markdown("---")
        
        st.subheader("ğŸ“ æ•°æ®èŒƒå›´è¦æ±‚")
        
        st.info("""
        **æœ€å°è¦æ±‚ï¼š**
        - è‡³å°‘ 2 ä¸ªå­¦ç”Ÿ
        - æ¯ä¸ªå­¦ç”Ÿè‡³å°‘ 5 æ¬¡ç­”é¢˜
        - è‡³å°‘ 2 ä¸ªä¸åŒçš„çŸ¥è¯†ç‚¹
        - è‡³å°‘ 10 é“ä¸åŒçš„é¢˜ç›®
        
        **æ¨èé…ç½®ï¼š**
        - 10-1000 ä¸ªå­¦ç”Ÿ
        - æ¯ä¸ªå­¦ç”Ÿ 20-200 æ¬¡ç­”é¢˜
        - 5-100 ä¸ªçŸ¥è¯†ç‚¹
        - 50-50000 é“é¢˜ç›®
        """)
        
        st.markdown("---")
        
        uploaded_file = st.file_uploader(
            "é€‰æ‹© CSV æ–‡ä»¶",
            type=['csv'],
            help="è¯·ä¸Šä¼ ç¬¦åˆæ ¼å¼è¦æ±‚çš„ CSV æ–‡ä»¶"
        )
        
        if uploaded_file is None:
            st.info("ğŸ’¡ è¯·ä¸Šä¼  CSV æ–‡ä»¶å¼€å§‹")
        else:
            try:
                df_uploaded = pd.read_csv(uploaded_file)
                
                st.subheader("ğŸ“Š æ•°æ®é¢„è§ˆ")
                st.dataframe(df_uploaded.head(10))
                
                st.subheader("âœ… æ•°æ®éªŒè¯")
                
                required_columns = ['user_id', 'item_id', 'correct', 'skill_id']
                missing_columns = [col for col in required_columns if col not in df_uploaded.columns]
                
                if missing_columns:
                    st.error(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {', '.join(missing_columns)}")
                else:
                    st.success("âœ… æ‰€æœ‰å¿…éœ€åˆ—éƒ½å­˜åœ¨")
                
                num_users = df_uploaded['user_id'].nunique()
                num_items = df_uploaded['item_id'].nunique()
                num_skills = df_uploaded['skill_id'].nunique()
                num_records = len(df_uploaded)
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("å­¦ç”Ÿæ•°é‡", num_users)
                with col2:
                    st.metric("é¢˜ç›®æ•°é‡", num_items)
                with col3:
                    st.metric("çŸ¥è¯†ç‚¹æ•°é‡", num_skills)
                with col4:
                    st.metric("ç­”é¢˜è®°å½•æ•°", num_records)
                
                st.subheader("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
                
                checks = []
                
                if num_users < 2:
                    checks.append(("âŒ", "å­¦ç”Ÿæ•°é‡å°‘äº 2 ä¸ª"))
                else:
                    checks.append(("âœ…", f"å­¦ç”Ÿæ•°é‡: {num_users} ä¸ª"))
                
                records_per_user = df_uploaded.groupby('user_id').size()
                min_records = records_per_user.min()
                if min_records < 5:
                    checks.append(("âŒ", f"æœ‰å­¦ç”Ÿç­”é¢˜è®°å½•å°‘äº 5 æ¬¡ï¼ˆæœ€å°‘ {min_records} æ¬¡ï¼‰"))
                else:
                    checks.append(("âœ…", f"æ¯ä¸ªå­¦ç”Ÿè‡³å°‘ {min_records} æ¬¡ç­”é¢˜"))
                
                if num_skills < 2:
                    checks.append(("âŒ", "çŸ¥è¯†ç‚¹æ•°é‡å°‘äº 2 ä¸ª"))
                else:
                    checks.append(("âœ…", f"çŸ¥è¯†ç‚¹æ•°é‡: {num_skills} ä¸ª"))
                
                if num_items < 10:
                    checks.append(("âŒ", "é¢˜ç›®æ•°é‡å°‘äº 10 é“"))
                else:
                    checks.append(("âœ…", f"é¢˜ç›®æ•°é‡: {num_items} é“"))
                
                if df_uploaded['correct'].isin([0, 1]).all():
                    checks.append(("âœ…", "correct åˆ—åªåŒ…å« 0 å’Œ 1"))
                else:
                    checks.append(("âŒ", "correct åˆ—åŒ…å«é 0 æˆ– 1 çš„å€¼"))
                
                for status, message in checks:
                    st.write(f"{status} {message}")
                
                all_passed = all("âœ…" in status for status, _ in checks)
                
                st.markdown("---")
                
                if all_passed:
                    st.success("ğŸ‰ æ•°æ®éªŒè¯é€šè¿‡ï¼å¯ä»¥ä¿å­˜")
                    
                    dataset_name = st.text_input(
                        "æ•°æ®é›†åç§°",
                        value="custom_dataset",
                        help="ç”¨äºæ ‡è¯†è¿™ä¸ªæ•°æ®é›†",
                        key="dataset_name_input"
                    )
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if st.button("ğŸ’¾ ä¿å­˜æ•°æ®", type="primary", key="save_data_btn"):
                            os.makedirs('data', exist_ok=True)
                            os.makedirs(f'data/{dataset_name}', exist_ok=True)
                            
                            save_path = f'data/{dataset_name}/preprocessed_data.csv'
                            df_uploaded.to_csv(save_path, sep='\t', index=False)
                            
                            st.success(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°: {save_path}")
                            st.info("ğŸ’¡ åˆ·æ–°é¡µé¢åï¼Œå¯ä»¥åœ¨å·¦ä¾§é€‰æ‹©è¿™ä¸ªæ•°æ®é›†äº†ï¼")
                            st.balloons()
                    
                    with col2:
                        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ•°æ®", key="clear_data_btn"):
                            if os.path.exists(f'data/{dataset_name}'):
                                import shutil
                                shutil.rmtree(f'data/{dataset_name}')
                                st.success(f"âœ… å·²æ¸…é™¤æ•°æ®é›†: {dataset_name}")
                else:
                    st.error("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œè¦æ±‚")
                    st.warning("ğŸ’¡ æç¤ºï¼šç¡®ä¿æ‰€æœ‰æ£€æŸ¥é¡¹éƒ½é€šè¿‡åå†ä¸Šä¼ ")
                
                st.markdown("---")
                
                st.subheader("ğŸ“ æ•°æ®ç¤ºä¾‹")
                st.code("""
user_id,item_id,timestamp,correct,skill_id
0,5504,20964177,1,206
0,5479,20964214,0,206
0,5466,20964236,1,206
0,5515,20964257,1,206
0,5491,20964272,0,206
0,5472,20964349,1,206
0,5490,20964372,1,206
0,5508,20964388,1,206
0,1754,20964422,1,195
0,2803,20964440,1,195
                """, language="csv")
            
            except Exception as e:
                st.error(f"âŒ è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                st.info("ğŸ’¡ è¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯æœ‰æ•ˆçš„ CSV æ–‡ä»¶")
    
    elif upload_type == "æ‹ç…§/å›¾ç‰‡ä¸Šä¼ ":
        st.subheader("ğŸ“· æ‹ç…§ä¸Šä¼ é¢˜ç›®")
        st.info("ğŸ’¡ è¯·æ‹æ‘„æ¸…æ™°çš„é¢˜ç›®å›¾ç‰‡ï¼Œç¡®ä¿æ–‡å­—æ¸…æ™°å¯è§")
        
        # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†å¿…è¦çš„åº“
        image_available = False
        try:
            # å°è¯•å¯¼å…¥PILåº“ï¼ˆç”¨äºå›¾åƒå¤„ç†ï¼‰
            from PIL import Image
            image_available = True
        except ImportError as e:
            st.error(f"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {str(e)}")
            st.info("ğŸ’¡ è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
            st.code("pip install pillow", language="bash")
            st.info("ğŸ’¡ æˆ–è€…ï¼Œå¦‚æœæ‚¨åªéœ€è¦ä¸Šä¼ CSVæ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨'CSV æ–‡ä»¶ä¸Šä¼ 'åŠŸèƒ½")
        
        if image_available:
            # å›¾ç‰‡ä¸Šä¼ åŠŸèƒ½
            image_file = st.file_uploader(
                "é€‰æ‹©å›¾ç‰‡æ–‡ä»¶",
                type=['jpg', 'jpeg', 'png'],
                help="è¯·ä¸Šä¼ åŒ…å«é¢˜ç›®çš„å›¾ç‰‡æ–‡ä»¶"
            )
            
            if image_file is not None:
                try:
                    # è¯»å–å›¾ç‰‡
                    image = Image.open(image_file)
                    
                    # æ˜¾ç¤ºå›¾ç‰‡é¢„è§ˆ
                    st.image(image, caption="ä¸Šä¼ çš„å›¾ç‰‡", width='stretch')
                    
                    # OCRè¯†åˆ«åŠŸèƒ½
                    st.subheader("ğŸ” OCRè¯†åˆ«")
                    
                    # æ£€æŸ¥OCRæ˜¯å¦å¯ç”¨
                    ocr_enabled = st.checkbox("å¯ç”¨OCRè‡ªåŠ¨è¯†åˆ«", value=True, help="è‡ªåŠ¨è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹")
                    
                    recognized_text = ""
                    if ocr_enabled:
                        # é€‰æ‹©OCRå¼•æ“
                        ocr_engine_type = st.selectbox(
                            "é€‰æ‹©OCRå¼•æ“",
                            ["PaddleOCRï¼ˆå¿«é€Ÿï¼Œé€‚åˆæ–‡å­—è¯†åˆ«ï¼‰", "PaddleOCR-VLï¼ˆå¼ºå¤§ï¼Œé€‚åˆå¤æ‚æ–‡æ¡£ï¼‰"],
                            help="PaddleOCRï¼šå¿«é€Ÿè½»é‡ï¼Œé€‚åˆé¢˜ç›®æ–‡å­—è¯†åˆ«\nPaddleOCR-VLï¼šåŠŸèƒ½å¼ºå¤§ï¼Œé€‚åˆè¡¨æ ¼ã€å…¬å¼ã€å›¾è¡¨ç­‰å¤æ‚å†…å®¹"
                        )
                        
                        try:
                            # è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨æ¨¡å‹æºæ£€æŸ¥
                            import os
                            os.environ['PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK'] = 'True'
                            
                            # å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–å¼•æ“ï¼Œé¿å…çŠ¶æ€æ··ä¹±
                            import os
                            
                            # æ˜¾ç¤ºåŠ è½½æç¤º
                            with st.spinner("æ­£åœ¨åˆå§‹åŒ–OCRå¼•æ“..."):
                                # æ¸…é™¤æ—§çš„å¼•æ“å®ä¾‹
                                st.session_state.ocr_engine = None
                                st.session_state.ocr_engine_type = ocr_engine_type
                                
                                # åˆå§‹åŒ–OCRå¼•æ“
                                if "PaddleOCR-VL" in ocr_engine_type:
                                    # ä½¿ç”¨PaddleOCR-VL
                                    try:
                                        from paddleocr import PaddleOCRVL
                                        st.session_state.ocr_engine = PaddleOCRVL()
                                        st.success("âœ… PaddleOCR-VLå¼•æ“å·²åŠ è½½")
                                    except ImportError:
                                        st.error("âŒ PaddleOCR-VLåº“æœªå®‰è£…")
                                        st.info("ğŸ’¡ è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
                                        st.code("pip install paddleocr", language="bash")
                                        ocr_enabled = False
                                    except Exception as e:
                                        st.error(f"âŒ åˆå§‹åŒ–PaddleOCR-VLæ—¶å‡ºé”™: {str(e)}")
                                        st.info("ğŸ’¡ PaddleOCR-VLéœ€è¦ä¸‹è½½è¾ƒå¤§çš„æ¨¡å‹æ–‡ä»¶ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´å’Œç½‘ç»œè¿æ¥")
                                        ocr_enabled = False
                                else:
                                    # ä½¿ç”¨PaddleOCR
                                    try:
                                        # å…ˆå¯¼å…¥PaddlePaddleå¹¶è®¾ç½®é…ç½®ï¼Œé¿å…ä¸PyTorchå†²çª
                                        import paddle
                                        # è®¾ç½®PaddlePaddleçš„GPUé…ç½®
                                        paddle.device.set_device('gpu:0' if paddle.device.is_compiled_with_cuda() else 'cpu')
                                        
                                        from paddleocr import PaddleOCR
                                        import numpy as np
                                        
                                        app_dir = os.path.dirname(os.path.abspath(__file__))
                                        model_dir = os.path.join(app_dir, 'models')
                                        
                                        if os.path.exists(model_dir):
                                            # æ£€æŸ¥æ¨¡å‹ç›®å½•ç»“æ„
                                            det_model_path = os.path.join(model_dir, 'ch_PP-OCRv4_det_infer')
                                            rec_model_path = os.path.join(model_dir, 'ch_PP-OCRv4_rec_infer')
                                            cls_model_path = os.path.join(model_dir, 'ch_PP-OCRv4_cls_infer')
                                            
                                            # å¤„ç†åµŒå¥—ç›®å½•ç»“æ„ï¼šå¦‚æœå­˜åœ¨åµŒå¥—ç›®å½•ï¼Œä½¿ç”¨åµŒå¥—ç›®å½•
                                            if os.path.exists(os.path.join(det_model_path, 'ch_PP-OCRv4_det_infer')):
                                                det_model_path = os.path.join(det_model_path, 'ch_PP-OCRv4_det_infer')
                                            if os.path.exists(os.path.join(rec_model_path, 'ch_PP-OCRv4_rec_infer')):
                                                rec_model_path = os.path.join(rec_model_path, 'ch_PP-OCRv4_rec_infer')
                                            if os.path.exists(os.path.join(cls_model_path, 'ch_PP-OCRv4_cls_infer')):
                                                cls_model_path = os.path.join(cls_model_path, 'ch_PP-OCRv4_cls_infer')
                                            
                                            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                                            det_model_exists = os.path.exists(os.path.join(det_model_path, 'inference.pdmodel'))
                                            rec_model_exists = os.path.exists(os.path.join(rec_model_path, 'inference.pdmodel'))
                                            
                                            if det_model_exists and rec_model_exists:
                                                # ä½¿ç”¨PaddleOCR 3.0.0çš„APIï¼Œè®©PaddleOCRè‡ªåŠ¨å¤„ç†æ¨¡å‹è·¯å¾„
                                                # è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨æ¨¡å‹æºæ£€æŸ¥
                                                import os
                                                os.environ['PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK'] = 'True'
                                                os.environ['PADDLEOCR_OFFLINE'] = 'True'
                                                
                                                # å°è¯•ä½¿ç”¨PaddleOCR 3.0.0çš„API
                                                try:
                                                    # ç®€åŒ–å‚æ•°ï¼Œè®©PaddleOCRè‡ªåŠ¨å¤„ç†æ¨¡å‹
                                                    st.session_state.ocr_engine = PaddleOCR(
                                                        lang='ch'
                                                    )
                                                    st.success("âœ… PaddleOCRå¼•æ“å·²åŠ è½½ï¼ˆé»˜è®¤æ¨¡å‹ï¼‰")
                                                    st.info("ğŸ’¡ ä½¿ç”¨PP-OCRv5_serveræœ€é«˜ç²¾åº¦æ¨¡å‹")
                                                except Exception as e:
                                                    # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä¸æŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œè®©PaddleOCRè‡ªåŠ¨ä¸‹è½½
                                                    st.warning(f"âš ï¸ ä½¿ç”¨æœ¬åœ°æ¨¡å‹å¤±è´¥: {str(e)}")
                                                    st.info("ğŸ’¡ å°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼ˆPP-OCRv5_serverï¼Œæœ€é«˜ç²¾åº¦ï¼‰...")
                                                    st.session_state.ocr_engine = PaddleOCR(
                                                        use_angle_cls=True,
                                                        lang='ch',
                                                        det_db_thresh=0.3,
                                                        det_db_box_thresh=0.5,
                                                        det_db_unclip_ratio=1.5
                                                    )
                                                    st.success("âœ… PaddleOCRå¼•æ“å·²åŠ è½½ï¼ˆPP-OCRv5_serveræœ€é«˜ç²¾åº¦æ¨¡å‹ï¼‰")
                                            else:
                                                st.error("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
                                                st.info(f"ğŸ’¡ æ£€æµ‹åˆ°çš„æ¨¡å‹è·¯å¾„ï¼š")
                                                st.info(f"   - æ£€æµ‹æ¨¡å‹: {det_model_path} {'âœ…' if det_model_exists else 'âŒ'}")
                                                st.info(f"   - è¯†åˆ«æ¨¡å‹: {rec_model_path} {'âœ…' if rec_model_exists else 'âŒ'}")
                                                st.info("ğŸ’¡ è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶æ­£ç¡®è§£å‹")
                                        else:
                                            # ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼Œä½†è®¾ç½®ç¯å¢ƒå˜é‡ç¦ç”¨ç½‘ç»œè¯·æ±‚
                                            import os
                                            os.environ['PADDLE_PDX_DISABLE_MODEL_SOURCE_CHECK'] = 'True'
                                            os.environ['PADDLEOCR_OFFLINE'] = 'True'
                                            try:
                                                st.session_state.ocr_engine = PaddleOCR(
                                                    use_angle_cls=True,
                                                    lang='ch',
                                                    det_db_thresh=0.3,
                                                    det_db_box_thresh=0.5,
                                                    det_db_unclip_ratio=1.5
                                                )
                                                st.success("âœ… PaddleOCRå¼•æ“å·²åŠ è½½ï¼ˆPP-OCRv5_serveræœ€é«˜ç²¾åº¦æ¨¡å‹ï¼‰")
                                            except Exception as e:
                                                st.error(f"âŒ åˆå§‹åŒ–PaddleOCRé»˜è®¤æ¨¡å‹æ—¶å‡ºé”™: {str(e)}")
                                                st.info("ğŸ’¡ OCRåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œå°†è‡ªåŠ¨åˆ‡æ¢åˆ°æ‰‹åŠ¨è¾“å…¥æ¨¡å¼")
                                                ocr_enabled = False
                                    except Exception as e:
                                        st.error(f"âŒ åˆå§‹åŒ–PaddleOCRæ—¶å‡ºé”™: {str(e)}")
                                        st.info("ğŸ’¡ è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦æ­£ç¡®è§£å‹åˆ°modelsç›®å½•")
                                        # å½“OCRåˆå§‹åŒ–å¤±è´¥æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æ‰‹åŠ¨è¾“å…¥æ¨¡å¼
                                        st.info("ğŸ’¡ OCRåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨ï¼Œå°†è‡ªåŠ¨åˆ‡æ¢åˆ°æ‰‹åŠ¨è¾“å…¥æ¨¡å¼")
                                        st.info("ğŸ’¡ ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½ä»ç„¶æ­£å¸¸å¯ç”¨")
                                        ocr_enabled = False
                            
                            # ä½¿ç”¨OCRè¯†åˆ«æ–‡å­—
                            with st.spinner("æ­£åœ¨è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—..."):
                                if "PaddleOCR-VL" in ocr_engine_type:
                                    # ä½¿ç”¨PaddleOCR-VLè¯†åˆ«
                                    import tempfile
                                    import os
                                    
                                    # ä¿å­˜å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.png') as tmp:
                                        image.save(tmp.name)
                                        tmp_path = tmp.name
                                    
                                    try:
                                        result = st.session_state.ocr_engine.predict(tmp_path)
                                        
                                        # æå–è¯†åˆ«ç»“æœ
                                        text_lines = []
                                        for res in result:
                                            if hasattr(res, 'rec_texts'):
                                                text_lines.extend(res.rec_texts)
                                            elif hasattr(res, 'text'):
                                                text_lines.append(res.text)
                                        
                                        recognized_text = '\n'.join(text_lines)
                                    finally:
                                        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                                        if os.path.exists(tmp_path):
                                            os.unlink(tmp_path)
                                else:
                                    # ä½¿ç”¨PaddleOCRè¯†åˆ«
                                    import numpy as np
                                    # è°ƒæ•´å›¾ç‰‡å¤§å°ä»¥æé«˜è¯†åˆ«ç‡
                                    from PIL import Image
                                    
                                    # è°ƒæ•´å›¾ç‰‡å¤§å°ï¼Œç¡®ä¿æ–‡å­—æ¸…æ™°
                                    max_size = 1024
                                    width, height = image.size
                                    if max(width, height) > max_size:
                                        ratio = max_size / max(width, height)
                                        new_width = int(width * ratio)
                                        new_height = int(height * ratio)
                                        image = image.resize((new_width, new_height), Image.LANCZOS)
                                    
                                    # ä½¿ç”¨PaddleOCRè¯†åˆ«
                                    result = st.session_state.ocr_engine.ocr(np.array(image))
                                    
                                    # æå–è¯†åˆ«ç»“æœ
                                    text_lines = []
                                    try:
                                        if result:
                                            st.info(f"ğŸ“Š OCRç»“æœç±»å‹: {type(result)}")
                                            if isinstance(result, list):
                                                st.info(f"ğŸ“Š OCRç»“æœé•¿åº¦: {len(result)}")
                                                for i, item in enumerate(result):
                                                    st.info(f"ğŸ“Š ç¬¬{i}é¡¹ç±»å‹: {type(item)}")
                                                    if isinstance(item, list):
                                                        st.info(f"ğŸ“Š ç¬¬{i}é¡¹æ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(item)}")
                                                        if len(item) > 0:
                                                            st.info(f"ğŸ“Š ç¬¬{i}é¡¹ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(item[0])}")
                                                            st.info(f"ğŸ“Š ç¬¬{i}é¡¹ç¬¬ä¸€ä¸ªå…ƒç´ å†…å®¹: {item[0]}")
                                                        # å¤„ç† PP-OCRv5_server çš„è¿”å›æ ¼å¼
                                                        for j, line in enumerate(item):
                                                            st.info(f"ğŸ“Š ç¬¬{i}-{j}è¡Œç±»å‹: {type(line)}")
                                                            st.info(f"ğŸ“Š ç¬¬{i}-{j}è¡Œé•¿åº¦: {len(line) if hasattr(line, '__len__') else 'N/A'}")
                                                            if isinstance(line, (list, tuple)):
                                                                st.info(f"ğŸ“Š ç¬¬{i}-{j}è¡Œå†…å®¹: {line}")
                                                                # å°è¯•ä¸åŒçš„ç»“æœæ ¼å¼
                                                                if len(line) >= 2:
                                                                    if isinstance(line[1], (list, tuple)) and len(line[1]) >= 1:
                                                                        text_lines.append(line[1][0])
                                                                        st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {line[1][0]}")
                                                                    elif isinstance(line[1], str):
                                                                        text_lines.append(line[1])
                                                                        st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {line[1]}")
                                                                elif len(line) >= 1:
                                                                    if isinstance(line[0], str):
                                                                        text_lines.append(line[0])
                                                                        st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {line[0]}")
                                                    elif hasattr(item, 'rec_texts'):
                                                        rec_texts = item.rec_texts
                                                        st.info(f"ğŸ“Š ç¬¬{i}é¡¹rec_textsç±»å‹: {type(rec_texts)}")
                                                        st.info(f"ğŸ“Š ç¬¬{i}é¡¹rec_textså†…å®¹: {rec_texts}")
                                                        if isinstance(rec_texts, list):
                                                            text_lines.extend(rec_texts)
                                                            for text in rec_texts:
                                                                st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {text}")
                                                        elif isinstance(rec_texts, str):
                                                            text_lines.append(rec_texts)
                                                            st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {rec_texts}")
                                                    elif isinstance(item, dict):
                                                        st.info(f"ğŸ“Š ç¬¬{i}é¡¹æ˜¯å­—å…¸ï¼Œé”®: {list(item.keys())}")
                                                        if 'text' in item:
                                                            text_lines.append(item['text'])
                                                            st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {item['text']}")
                                                        elif 'rec_texts' in item:
                                                            text_lines.extend(item['rec_texts'])
                                                            st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {item['rec_texts']}")
                                                    elif hasattr(item, 'text'):
                                                        text_lines.append(item.text)
                                                        st.info(f"ğŸ“Š è¯†åˆ«åˆ°æ–‡å­—: {item.text}")
                                                    else:
                                                        st.info(f"ğŸ“Š ç¬¬{i}é¡¹å†…å®¹: {item}")
                                    except Exception as e:
                                        st.warning(f"âš ï¸ è§£æOCRç»“æœæ—¶å‡ºé”™: {str(e)}")
                                    
                                    recognized_text = '\n'.join(text_lines)
                                    st.info(f"ğŸ“Š æœ€ç»ˆè¯†åˆ«ç»“æœ: {recognized_text}")
                            
                            # æ˜¾ç¤ºè¯†åˆ«ç»“æœ
                            if recognized_text.strip():
                                st.success("âœ… è¯†åˆ«æˆåŠŸï¼")
                                st.text_area("è¯†åˆ«çš„é¢˜ç›®å†…å®¹", recognized_text, height=200, key="ocr_result")
                                st.info("ğŸ’¡ æ‚¨å¯ä»¥ç¼–è¾‘è¯†åˆ«ç»“æœï¼Œç¡®ä¿å†…å®¹å‡†ç¡®")
                            else:
                                st.warning("âš ï¸ æœªèƒ½è¯†åˆ«å‡ºæ–‡å­—ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥")
                                
                        except ImportError as e:
                            st.error("âŒ OCRåº“æœªå®‰è£…")
                            st.info("ğŸ’¡ è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…:")
                            st.code("pip install paddleocr paddlepaddle", language="bash")
                            ocr_enabled = False
                        except Exception as e:
                            error_msg = str(e)
                            if "model source" in error_msg or "download model" in error_msg or "network" in error_msg or "æ‹’ç»è®¿é—®" in error_msg:
                                st.error("âš ï¸ OCRåŠŸèƒ½æš‚æ—¶ä¸å¯ç”¨")
                                st.warning("ğŸ“‹ åŸå› åˆ†æï¼š")
                                st.warning("   1. PaddleOCRéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶åˆ°ç³»ç»Ÿç›®å½•")
                                st.warning("   2. å¯èƒ½å­˜åœ¨æƒé™æˆ–ç½‘ç»œè®¿é—®é™åˆ¶")
                                st.warning("   3. è¿™æ˜¯Windowsç³»ç»Ÿçš„å¸¸è§é—®é¢˜")
                                st.info("ğŸ’¡ æœ€ä½³è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æœ¬åœ°æ¨¡å‹æ–‡ä»¶")
                                st.info("   1. ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆ3ä¸ªæ–‡ä»¶ï¼Œæ€»å…±çº¦18Mï¼‰ï¼š")
                                st.code("https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_det_infer.tar", language="text")
                                st.code("https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_rec_infer.tar", language="text")
                                st.code("https://paddleocr.bj.bcebos.com/PP-OCRv4/chinese/ch_PP-OCRv4_cls_infer.tar", language="text")
                                st.info("   2. è§£å‹åˆ°ä»¥ä¸‹ç›®å½•ï¼š")
                                st.code("TBKT4/models/", language="text")
                                st.info("   3. è§£å‹åç›®å½•ç»“æ„åº”è¯¥æ˜¯ï¼š")
                                st.code("TBKT4/models/ch_PP-OCRv4_det_infer/", language="text")
                                st.code("TBKT4/models/ch_PP-OCRv4_rec_infer/", language="text")
                                st.code("TBKT4/models/ch_PP-OCRv4_cls_infer/", language="text")
                                st.success("âœ… ç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½å®Œå…¨æ­£å¸¸ï¼Œä¸å—å½±å“ï¼")
                                st.info("ğŸ“š å¯ç”¨åŠŸèƒ½ï¼š")
                                st.info("   - ä¸ªæ€§åŒ–å­¦ä¹ æ¨è")
                                st.info("   - å­¦ä¹ è·¯å¾„ä¼˜åŒ–")
                                st.info("   - çŸ¥è¯†è¿½è¸ªåˆ†æ")
                                st.info("   - æ•™è‚²è¯„ä¼°")
                            else:
                                st.warning(f"âš ï¸ OCRè¯†åˆ«å¤±è´¥: {error_msg}")
                            st.info("ğŸ’¡ æ‚¨å¯ä»¥æ‰‹åŠ¨è¾“å…¥é¢˜ç›®å†…å®¹")
                            ocr_enabled = False
                    
                    # é¢˜ç›®ä¿¡æ¯è¾“å…¥
                    st.subheader("ğŸ“ é¢˜ç›®ä¿¡æ¯")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        skill_id = st.number_input("çŸ¥è¯†ç‚¹ ID", min_value=0, value=0, help="é¢˜ç›®æ‰€å±çš„çŸ¥è¯†ç‚¹ ID")
                    with col2:
                        difficulty = st.selectbox("éš¾åº¦çº§åˆ«", ["ç®€å•", "ä¸­ç­‰", "å›°éš¾"], help="é¢˜ç›®çš„éš¾åº¦çº§åˆ«")
                    
                    # é¢˜ç›®å†…å®¹è¾“å…¥ï¼ˆå¦‚æœOCRè¯†åˆ«æˆåŠŸï¼Œé¢„å¡«å……è¯†åˆ«ç»“æœï¼‰
                    default_text = recognized_text if ocr_enabled and recognized_text.strip() else ""
                    manual_text = st.text_area("é¢˜ç›®å†…å®¹", default_text, height=200, help="è¯·è¾“å…¥æˆ–ç¼–è¾‘é¢˜ç›®å†…å®¹")
                    
                    # ä¿å­˜é¢˜ç›®
                    dataset_name = st.text_input(
                        "æ•°æ®é›†åç§°",
                        value="custom_images",
                        help="ç”¨äºæ ‡è¯†è¿™ä¸ªæ•°æ®é›†",
                        key="image_dataset_name"
                    )
                    
                    if st.button("ğŸ’¾ ä¿å­˜é¢˜ç›®", type="primary", key="save_image_btn"):
                        # åˆ›å»ºæ•°æ®é›†ç›®å½•
                        os.makedirs('data', exist_ok=True)
                        os.makedirs(f'data/{dataset_name}', exist_ok=True)
                        
                        # ä¿å­˜å›¾ç‰‡
                        image_path = f'data/{dataset_name}/images'
                        os.makedirs(image_path, exist_ok=True)
                        
                        image_filename = f"{len(os.listdir(image_path)) + 1}.jpg"
                        image.save(f"{image_path}/{image_filename}")
                        
                        # ä¿å­˜é¢˜ç›®ä¿¡æ¯
                        import json
                        questions_path = f'data/{dataset_name}/questions.json'
                        
                        if os.path.exists(questions_path):
                            with open(questions_path, 'r', encoding='utf-8') as f:
                                questions = json.load(f)
                        else:
                            questions = []
                        
                        new_question = {
                            "id": len(questions) + 1,
                            "content": manual_text,
                            "skill_id": int(skill_id),
                            "difficulty": difficulty,
                            "image": image_filename
                        }
                        
                        questions.append(new_question)
                        
                        with open(questions_path, 'w', encoding='utf-8') as f:
                            json.dump(questions, f, ensure_ascii=False, indent=2)
                        
                        st.success(f"âœ… é¢˜ç›®å·²ä¿å­˜åˆ°: {questions_path}")
                        st.info("ğŸ’¡ æ‚¨å¯ä»¥ç»§ç»­ä¸Šä¼ æ›´å¤šé¢˜ç›®")
                        st.balloons()
                        
                except Exception as e:
                    error_msg = str(e)
                    st.error(f"âŒ å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {error_msg}")
                    st.info("ğŸ’¡ è¯·ç¡®ä¿ä¸Šä¼ çš„æ˜¯æœ‰æ•ˆçš„å›¾ç‰‡æ–‡ä»¶")
        else:
            st.info("ğŸ’¡ è¯·ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶å¼€å§‹")
        
        st.markdown("---")
        st.subheader("ğŸ“ ä½¿ç”¨æç¤º")
        st.write("1. **æ‹æ‘„æŠ€å·§**ï¼šç¡®ä¿å…‰çº¿å……è¶³ï¼Œæ–‡å­—æ¸…æ™°å¯è§")
        st.write("2. **å›¾ç‰‡è¦æ±‚**ï¼šå°½é‡åªåŒ…å«é¢˜ç›®å†…å®¹ï¼Œé¿å…å…¶ä»–å¹²æ‰°")
        st.write("3. **OCRè¯†åˆ«**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—ï¼Œæ‚¨å¯ä»¥ç¼–è¾‘è¯†åˆ«ç»“æœ")
        st.write("4. **æ‰‹åŠ¨è¾“å…¥**ï¼šå¦‚æœOCRè¯†åˆ«ä¸å‡†ç¡®ï¼Œå¯ä»¥æ‰‹åŠ¨è¾“å…¥é¢˜ç›®å†…å®¹")
        st.write("5. **æ‰¹é‡ä¸Šä¼ **ï¼šæ‚¨å¯ä»¥å¤šæ¬¡ä¸Šä¼ å›¾ç‰‡æ¥åˆ›å»ºé¢˜åº“")
    
    elif upload_type == "è‡ªå®šä¹‰åç§°":
        st.subheader("ğŸ·ï¸ è‡ªå®šä¹‰åç§°")
        st.info("ğŸ’¡ ä¸ºå­¦ç”Ÿã€é¢˜ç›®å’ŒçŸ¥è¯†ç‚¹è®¾ç½®å‹å¥½çš„åç§°")
        
        name_type = st.radio(
            "é€‰æ‹©è¦è‡ªå®šä¹‰çš„ç±»å‹",
            ["å­¦ç”Ÿåç§°", "çŸ¥è¯†ç‚¹åç§°", "é¢˜ç›®åç§°"],
            horizontal=True
        )
        
        if name_type == "å­¦ç”Ÿåç§°":
            st.subheader("ğŸ‘¨â€ğŸ“ å­¦ç”Ÿåç§°è®¾ç½®")
            
            unique_users = sorted(df['user_id'].unique())
            selected_user_id = st.selectbox(
                "é€‰æ‹©å­¦ç”ŸID",
                unique_users,
                format_func=lambda x: f"{get_user_name(mappings, x)} (ID: {x})"
            )
            
            current_name = get_user_name(mappings, selected_user_id)
            new_name = st.text_input("è¾“å…¥æ–°åç§°", value=current_name)
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ä¿å­˜åç§°"):
                    set_user_name(mappings, selected_user_id, new_name)
                    save_mappings(mappings)
                    st.success(f"âœ… å·²ä¿å­˜: {new_name}")
                    st.rerun()
            
            with col2:
                if st.button("è‡ªåŠ¨ç”Ÿæˆåç§°"):
                    set_user_name(mappings, selected_user_id, f"å­¦ç”Ÿ{selected_user_id}")
                    save_mappings(mappings)
                    st.success(f"âœ… å·²é‡ç½®ä¸ºé»˜è®¤åç§°")
                    st.rerun()
        
        elif name_type == "çŸ¥è¯†ç‚¹åç§°":
            st.subheader("ğŸ“š çŸ¥è¯†ç‚¹åç§°è®¾ç½®")
            
            unique_skills = sorted(df['skill_id'].unique())
            selected_skill_id = st.selectbox(
                "é€‰æ‹©çŸ¥è¯†ç‚¹ID",
                unique_skills,
                format_func=lambda x: f"{get_skill_name(mappings, x)} (ID: {x})"
            )
            
            current_name = get_skill_name(mappings, selected_skill_id)
            new_name = st.text_input("è¾“å…¥æ–°åç§°", value=current_name)
            
            skill_data = df[df['skill_id'] == selected_skill_id]
            avg_accuracy = skill_data['correct'].mean()
            st.info(f"è¯¥çŸ¥è¯†ç‚¹å¹³å‡æ­£ç¡®ç‡: {avg_accuracy:.2%}")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                if st.button("ä¿å­˜åç§°"):
                    set_skill_name(mappings, selected_skill_id, new_name)
                    save_mappings(mappings)
                    st.success(f"âœ… å·²ä¿å­˜: {new_name}")
                    st.rerun()
            
            with col2:
                if st.button("è‡ªåŠ¨ç”Ÿæˆåç§°"):
                    set_skill_name(mappings, selected_skill_id, f"çŸ¥è¯†ç‚¹{selected_skill_id}")
                    save_mappings(mappings)
                    st.success(f"âœ… å·²é‡ç½®ä¸ºé»˜è®¤åç§°")
                    st.rerun()
            
            with col3:
                if st.button("æ‰¹é‡ç”Ÿæˆåç§°"):
                    mappings = auto_generate_skill_names(df)
                    st.success(f"âœ… å·²ä¸ºæ‰€æœ‰çŸ¥è¯†ç‚¹ç”Ÿæˆåç§°")
                    st.rerun()
        
        elif name_type == "é¢˜ç›®åç§°":
            st.subheader("ğŸ“ é¢˜ç›®åç§°è®¾ç½®")
            
            unique_items = sorted(df['item_id'].unique())
            selected_item_id = st.selectbox(
                "é€‰æ‹©é¢˜ç›®ID",
                unique_items,
                format_func=lambda x: f"{get_item_name(mappings, x)} (ID: {x})"
            )
            
            current_name = get_item_name(mappings, selected_item_id)
            new_name = st.text_input("è¾“å…¥æ–°åç§°", value=current_name)
            
            col1, col2 = st.columns(2)
            with col1:
                if st.button("ä¿å­˜åç§°"):
                    set_item_name(mappings, selected_item_id, new_name)
                    save_mappings(mappings)
                    st.success(f"âœ… å·²ä¿å­˜: {new_name}")
                    st.rerun()
            
            with col2:
                if st.button("è‡ªåŠ¨ç”Ÿæˆåç§°"):
                    set_item_name(mappings, selected_item_id, f"é¢˜ç›®{selected_item_id}")
                    save_mappings(mappings)
                    st.success(f"âœ… å·²é‡ç½®ä¸ºé»˜è®¤åç§°")
                    st.rerun()
        
        st.markdown("---")
        st.subheader("ğŸ“Š å½“å‰åç§°æ˜ å°„")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("å­¦ç”Ÿåç§°æ•°", len(mappings["user_names"]))
        with col2:
            st.metric("çŸ¥è¯†ç‚¹åç§°æ•°", len(mappings["skill_names"]))
        with col3:
            st.metric("é¢˜ç›®åç§°æ•°", len(mappings["item_names"]))

st.markdown("---")
st.caption("Â© 2024 æ™ºèƒ½çŸ¥è¯†è¿½è¸ªç³»ç»Ÿ | åŸºäº SAKT å’Œ TSAKT æ¨¡å‹")